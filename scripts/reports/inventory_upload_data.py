
import json
#from lmfdb.inventory_app import inventory_helpers as ih
from lmfdb.base import getDBConnection
import lmfdb.inventory_app.inventory_helpers as ih
import lmfdb.inventory_app.lmfdb_inventory as inv
import lmfdb.inventory_app.inventory_db_inplace as invip
import lmfdb.inventory_app.inventory_db_core as invc

#TODO this should log to its own logger
#Routines to upload all inventory info from json files from the auto-generator

MAX_SZ = 10000

def upload_from_files(db, master_file_name, list_file_name):
    """Upload an entire inventory. CLOBBERS CONTENT
    
        db -- LMFDB connection to inventory database
        master_file_name -- path to report tool database structure file
        list_file_name -- path to file containing list of all additional inventory info files
    """

    decoder = json.JSONDecoder()
    structure_dat = decoder.decode(read_file(master_file_name))

    inv.log_dest.info("_____________________________________________________________________________________________")
    inv.log_dest.info("Processing autogenerated inventory")
    for DB_name in structure_dat:
        inv.log_dest.info("Uploading " + DB_name)
        _id = invc.set_db(db, DB_name)

        for coll_name in structure_dat[DB_name]:
            inv.log_dest.info("    Uploading collection "+coll_name)
            coll_entry = structure_dat[DB_name][coll_name]
            _c_id = invc.set_coll(db, _id['id'], coll_name, '', '')

            for field in coll_entry['fields']:
                invc.set_field(db, _c_id['id'], field, coll_entry['fields'][field])
            for record in coll_entry['records']:
                invc.set_record(db, _c_id['id'], coll_entry['records'][record])

    inv.log_dest.info("_____________________________________________________________________________________________")
    inv.log_dest.info("Processing additional inventory")
    file_list = read_list(list_file_name)
    last_db = ''
    for file in file_list:
        data = decoder.decode(read_file(file))
        record_name = ih.get_description_key(file)
        DB_name = record_name[0]
        if DB_name != last_db:
            inv.log_dest.info("Uploading " + DB_name)
            last_db = DB_name
        coll_name = record_name[1]
        inv.log_dest.info("    Uploading collection "+coll_name)
    
        db_entry = invc.get_db_id(db, DB_name)
        if not db_entry['exist']:
            #All dbs should have been added from the struc: if not is error
            inv.log_dest.error("ERROR: No inventory DB entry "+ DB_name)
            inv.log_dest.error("Cannot add descriptions")
            continue

        coll_entry = invc.get_coll_id(db, db_entry['id'], coll_name)
        if not coll_entry['exist']:
            #All dbs should have been added from the struc: if not is error
            inv.log_dest.error("ERROR: No inventory collection entry " + DB_name +" "+ coll_name)
            inv.log_dest.error("Cannot add descriptions")
            continue

        split_data = extract_specials(data)

        #Insert the notes and info fields into the collection
        split_data[inv.STR_NOTES] = ih.blank_all_empty_fields(split_data[inv.STR_NOTES])
        inv.log_dest.debug(split_data[inv.STR_NOTES])
        split_data[inv.STR_INFO] = ih.blank_all_empty_fields(split_data[inv.STR_INFO])
        inv.log_dest.debug(split_data[inv.STR_INFO])
        _c_id = invc.set_coll(db, db_entry['id'], coll_name, split_data[inv.STR_NOTES], split_data[inv.STR_INFO])

        for field in split_data['data']:
            if not ih.is_record_name(split_data['data'][field]):
                invc.set_field(db, _c_id['id'], field, split_data['data'][field], type='human')
            else:
                invc.set_record(db, _c_id['id'], {'hash':field, 'name':split_data['data'][field]}, type='human')


def upload_collection_from_files(db, db_name, coll_name, master_file_name, json_file_name):
    """Freshly upload inventory for a single collection. CLOBBERS CONTENT
    
        db -- LMFDB connection to inventory database
        db_name -- Name of database this collection is in
        coll_name -- Name of collection to upload
        master_file_name -- path to report tool database structure file
        json_file_name -- path to file containing additional inventory data
    """

    decoder = json.JSONDecoder()
    structure_dat = decoder.decode(read_file(master_file_name))

    inv.log_dest.info("_____________________________________________________________________________________________")
    inv.log_dest.info("Uploading collection "+coll_name)

    try:
        coll_entry = structure_dat[db_name][coll_name]
        db_entry = get_db_id(db, db_name)
        if not db_entry['exist']:
            #All dbs should have been added from the struc: if not is error
            inv.log_dest.error("ERROR: No inventory DB entry "+ db_name)
            inv.log_dest.error("Cannot add descriptions")
            return

        _c_id = set_coll(db, db_entry['id'], coll_name, '', '')

        for field in coll_entry['fields']:
            invc.set_field(db, _c_id['id'], field, coll_entry['fields'][field])
        for record in coll_entry['records']:
            invc.set_record(db, _c_id['id'], coll_entry['records'][record])

    except Exception as e:
        inv.log_dest.error("Failed to refresh collection "+str(e))

    try:
        data = decoder.decode(read_file(json_file_name))
        
        split_data = extract_specials(data)

        #Insert the notes and info fields into the collection
        split_data[inv.STR_NOTES] = ih.blank_all_empty_fields(split_data[inv.STR_NOTES])
        inv.log_dest.debug(split_data[inv.STR_NOTES])
        split_data[inv.STR_INFO] = ih.blank_all_empty_fields(split_data[inv.STR_INFO])
        inv.log_dest.debug(split_data[inv.STR_INFO])
        _c_id = invc.set_coll(db, db_entry['id'], coll_name, split_data[inv.STR_NOTES], split_data[inv.STR_INFO])

        for field in split_data['data']:
            if not ih.is_record_name(split_data['data'][field]):
                invc.set_field(db, _c_id['id'], field, split_data['data'][field], type='human')
            else:
                invc.set_record(db, _c_id['id'], {'hash':field, 'name':split_data['data'][field]}, type='human')
    except Exception as e:
        inv.log_dest.error("Failed to refresh collection "+str(e))


def refresh_collection_structure(db, db_name, coll_name, master_file_name):
    """Refresh the structure description for a single collection
    
    Collection must exist. Any entered descriptions for keys which still exist are preserved. Removed or renamed keys will be dumped.
    db -- LMFDB connection to inventory database
    db_name -- Name of database this collection is in
    coll_name -- Name of collection to upload
    master_file_name -- path to new report tool database structure file
    """

    decoder = json.JSONDecoder()
    structure_dat = decode.decode(read_file(master_file_name))

    inv.log_dest.info("_____________________________________________________________________________________________")
    inv.log_dest.info("Refreshing collection "+coll_name)

    try:
        coll_entry = structure_dat[db_name][coll_name]
        db_entry = invc.get_db_id(db, db_name)
        if not db_entry['exist']:
            #All dbs should have been added from the struc: if not is error
            inv.log_dest.error("ERROR: No inventory DB entry "+ db_name)
            inv.log_dest.error("Cannot add descriptions")
            return

        _c_id = get_coll_id(db, db_entry['id'], coll_name)

        for field in coll_entry['fields']:
            invc.set_field(db, _c_id['id'], field, coll_entry['fields'][field])
        for record in coll_entry['records']:
            invc.set_record(db, _c_id['id'], coll_entry['records'][record])

    except Exception as e:
        inv.log_dest.error("Failed to refresh collection "+str(e))

    try:
        data = decoder.decode(read_file(json_file_name))
        
        split_data = extract_specials(data)

        #Insert the notes and info fields into the collection
        split_data[inv.STR_NOTES] = ih.blank_all_empty_fields(split_data[inv.STR_NOTES])
        inv.log_dest.debug(split_data[inv.STR_NOTES])
        split_data[inv.STR_INFO] = ih.blank_all_empty_fields(split_data[inv.STR_INFO])
        inv.log_dest.debug(split_data[inv.STR_INFO])
        _c_id = invc.update_coll(db, db_entry['id'], coll_name, split_data[inv.STR_NOTES], split_data[inv.STR_INFO])

        orphaned_keys = invc.trim_human_table(db, db_entry['id'], _c_id['id'])
    except Exception as e:
        inv.log_dest.error("Failed to refresh collection "+str(e))

def extract_specials(coll_entry):
    """ Split coll_entry into data and specials (notes, info etc) parts """
    notes = ''
    notes_entry = ''
    info = ''
    info_entry = ''
    for item in coll_entry:
        if item == inv.STR_NOTES:
            notes = item
            notes_entry = coll_entry[item]
        elif item == inv.STR_INFO:
            info = item
            info_entry = coll_entry[item]
    try:
        coll_entry.pop(notes)
        coll_entry.pop(info)
    except:
        pass
    return {inv.STR_NOTES:notes_entry, inv.STR_INFO: info_entry, 'data': coll_entry}

def read_file(filename):
    """Read entire file contents """
    with open(filename, 'r') as in_file:
        dat = in_file.read()
    return dat

def read_list(listfile):
    """Read file line-wise into list of lines """
    with open(listfile, 'r') as in_file:
        lines = in_file.read().splitlines()
    return lines

#End upload routines -----------------------------------------------------------------

#Table removal -----------------------------------------------------------------------
def delete_contents(db, tbl_name):
    """Delete contents of tbl_name """

    if not inv.validate_mongodb(db):
        raise TypeError("db does not match Inventory structure")
        return
    #Grab the possible table names
    tbl_names = inv.get_inv_table_names()
    if tbl_name in tbl_names:
        try:
            db[tbl_name].remove()
        except Exception as e:
            inv.log_dest.error("Error deleting "+ tbl_name+' '+ str(e))

def delete_all_tables(db):
    """ Delete all tables specified by inv """

    if not inv.validate_mongodb(db):
        raise TypeError("db does not match Inventory structure")
        return
    tbls = inv.get_inv_table_names()
    for tbl in tbls:
        try:
            delete_contents(db, tbl)
        except Exception as e:
            inv.log_dest.error("Error deleting "+ tbl + ' ' +str(e))

def delete_by_collection(inv_db, db_name, coll_name):
    """Remove collection entry and all its fields"""

    if not inv.validate_mongodb(inv_db):
        raise TypeError("db does not match Inventory structure")
        return

    try:
        _db_id = invc.get_db_id(inv_db, db_name)
        _c_id = invc.get_coll_id(inv_db, _db_id['id'], coll_name)
    except Exception as e:
        inv.log_dest.error("Error getting collection " + str(e))
        return {'err':True, 'id':0, 'exist':False}

    #Remove fields entries matching _c_id
    try:
        fields_tbl = inv.ALL_STRUC.get_fields('auto')[inv.STR_NAME]
        fields_fields = inv.ALL_STRUC.get_fields('auto')[inv.STR_CONTENT]
        rec_find = {fields_fields[1]:_c_id['id']}
        inv_db[fields_tbl].remove(rec_find)
    except Exception as e:
        inv.log_dest.error("Error removing fields " + str(e))

    try:
        fields_tbl = inv.ALL_STRUC.get_fields('human')[inv.STR_NAME]
        fields_fields = inv.ALL_STRUC.get_fields('human')[inv.STR_CONTENT]
        rec_find = {fields_fields[1]:_c_id['id']}
        inv_db[fields_tbl].remove(rec_find)
    except Exception as e:
        inv.log_dest.error("Error removing fields " + str(e))

    try:
        inv_db[inv.ALL_STRUC.coll_ids[inv.STR_NAME]].remove({'_id':_c_id['id']})
    except Exception as e:
        inv.log_dest.error("Error removing collection " + str(e))


#End table removal -----------------------------------------------------------------------

#Initial uploader routines ---------------------------------------------------------------
def fresh_upload(master_file_name, list_file_name):
    """Delete existing data and upload a fresh copy.

      Arguments :
      master_file_name -- path to structure file from report tool (e.g. lmfdb_structure.json)
      list_file_name -- path to file containing names of all json files to upload (one per collection)
    """
    got_client = inv.setup_internal_client()
    if not got_client:
        exit()
    db = inv.int_client[inv.get_inv_db_name()]

    #DELETE all existing inventory!!!
    delete_all_tables(db)

    upload_from_files(db, master_file_name, list_file_name)
    recreate_rollback_table(db, MAX_SZ)

def fresh_upload_coll(db_name, coll_name, master_file_name, json_file_name):
    """Delete existing data and upload a fresh copy for a single collection.

      Arguments :
      db_name -- name of database to refresh
      coll_name -- name of collection to refresh
      master_file_name -- path to structure file from report tool (entire or single collection)
      json_file_name -- path to additional json file for this collection
    """
    got_client = inv.setup_internal_client()
    if not got_client:
        exit()
    db = inv.int_client[inv.get_inv_db_name()]

    delete_by_collection(db, db_name, coll_name)
    upload_collection_from_files(db, db_name, coll_name, master_file_name, json_file_name)

def recreate_rollback_table(inv_db, sz):
    """Create anew the table for edit rollbacks
    
    Arguments :
    inv_db -- LMFDB db connection to inventory table
    sz -- Max size of the capped table
    If table exists, it is now deleted
    """
    try:
        table_name = inv.ALL_STRUC.rollback_human[inv.STR_NAME]
        coll = inv_db[table_name]
    except Exception as e:
        inv.log_dest.error("Error getting collection "+str(e))
        return {'err':True, 'id':0}
    fields = inv.ALL_STRUC.rollback_human[inv.STR_CONTENT]

    try:
        inv_db[table_name].drop()
    except:
        pass

    inv_db.create_collection(table_name, capped=True, size=sz)

if __name__ == "__main__":
    got_client = inv.setup_internal_client()
    if not got_client:
        exit()
    db = inv.int_client[inv.get_inv_db_name()]

    master_file_name = "./lmfdb/lmfdb_report_tool-master/intermediates/lmfdb_structure.json"
    list_file_name = "descr.txt"
    fresh_upload(master_file_name, list_file_name)

    tbls = inv.get_inv_table_names()
    for tbl in tbls:
        print(tbl)
        for item in db[tbl].find():
            try:
                print('   '+item['name'])
                print('       '+str(item['data']))
            except:
                pass


